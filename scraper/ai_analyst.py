import os
import time
import httpx
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(os.path.join(os.path.dirname(__file__), '..', '.env'))

# ---------------- CONFIGURATION ---------------- #
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "")
YOUR_SITE_URL = "https://polydelta.vercel.app"
APP_NAME = "PolyDelta Arbitrage"

# ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨ç¨³å®šçš„å…è´¹æ¨¡å‹
# å‚è€ƒ: https://openrouter.ai/docs/models
PRIMARY_MODEL = "google/gemini-2.0-flash-exp:free"  # Gemini Flash ä½œä¸ºä¸»è¦æ¨¡å‹
FALLBACK_MODEL = "meta-llama/llama-3.2-3b-instruct:free"  # Llama 3.2 ä½œä¸ºå¤‡ç”¨

# ä½¿ç”¨ httpx è®¾ç½®è¶…æ—¶
client = None
if OPENROUTER_API_KEY:
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=OPENROUTER_API_KEY,
        timeout=httpx.Timeout(60.0, connect=10.0)
    )

def generate_ai_report(match_data, is_championship=False):
    """
    ç”Ÿæˆ AI åˆ†ææŠ¥å‘Šã€‚
    ç­–ç•¥ï¼šä¼˜å…ˆå°è¯• DeepSeek R1 è¿›è¡Œæ·±åº¦æ¨ç†ï¼›å¦‚æœè¶…æ—¶æˆ–æŠ¥é”™ï¼Œåˆ‡æ¢ Gemini Flash è¿›è¡Œå¿«é€Ÿæ€»ç»“ã€‚
    """
    if not client:
        print("   âš ï¸ OPENROUTER_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡ AI åˆ†æ")
        return None

    ev = float(match_data.get('ev', 0))
    threshold = 0.05 if is_championship else 0.02

    # é—¨æ§›è¿‡æ»¤ï¼šåªæœ‰é«˜ä»·å€¼æœºä¼šæ‰åˆ†æ
    if ev < threshold:
        return None

    title = match_data.get('title', 'Unknown Match')
    web2_odds = match_data.get('web2_odds', 0)
    poly_price = match_data.get('polymarket_price', 0)
    ev_percent = ev * 100

    print(f"ğŸ§  AI Analyst observing: {title} (EV: +{ev_percent:.1f}%)")

    r1_system_prompt = "You are a professional Sports Arbitrage Analyst. Analyze the divergence between Bookmaker Odds (Smart Money) and Polymarket Price (Retail Sentiment). Focus on WHY the gap exists. Output strictly clean Markdown."
    user_content = f"Analyze arbitrage for: {title}. Web2 Odds: {web2_odds:.1f}%. Polymarket Price: {poly_price:.1f}%. Net EV: +{ev_percent:.1f}%. Provide: 1. The Divergence Cause 2. Risk Assessment 3. Verdict. Keep it concise (under 150 words)."

    # å°è¯•ä¸»è¦æ¨¡å‹ (Gemini Flash)
    try:
        time.sleep(1)  # é¿å…é€Ÿç‡é™åˆ¶
        result = call_llm(PRIMARY_MODEL, r1_system_prompt, user_content)
        if result:
            print(f"   âœ… Gemini Flash æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
            return result
    except Exception as e:
        print(f"   âš ï¸ Primary model error: {str(e)[:60]}...")

    # Fallback: Llama 3.2
    print(f"   ğŸ”„ Switching to Fallback (Llama 3.2)...")
    try:
        time.sleep(1)  # é¿å…é€Ÿç‡é™åˆ¶
        fallback_system_prompt = "You are a fast Trading Assistant. Give a quick TL;DR arbitrage analysis. Be direct and factual. Keep it under 100 words."
        result = call_llm(FALLBACK_MODEL, fallback_system_prompt, user_content)
        if result:
            print(f"   âœ… Llama 3.2 æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
            return result
    except Exception as e:
        print(f"   âŒ Fallback model error: {str(e)[:60]}...")

    return None

def call_llm(model, sys_prompt, user_prompt):
    """è°ƒç”¨ LLM API"""
    completion = client.chat.completions.create(
        extra_headers={"HTTP-Referer": YOUR_SITE_URL, "X-Title": APP_NAME},
        model=model,
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.7,
        max_tokens=500,
    )
    content = completion.choices[0].message.content

    # æ¸…æ´— DeepSeek çš„æ€ç»´é“¾
    if content and "<think>" in content:
        parts = content.split("</think>")
        if len(parts) > 1:
            content = parts[-1].strip()

    return content.replace("```markdown", "").replace("```", "").strip() if content else None
